---
id: s3
title: AWS S3
---

## Introduction

_AWS Simple Storage Service_ ([S3](https://aws.amazon.com/s3/?nc=sn&loc=0)) is an object storage service that offers industry leading scalability, availability, security and performance.
It allows data storage of any amount of data, commonly used as a data lake for big data applications which can now be easily integrated with monix.
   
## Dependency
 
 Add the following dependency:
 
 ```scala
 libraryDependencies += "io.monix" %% "monix-s3" % "0.5.0"
 ```
 
## Async Client
 
 This connector uses the _underlying_ `S3AsyncClient` from the [java aws sdk](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/package-summary.html),
 it will allow us to authenticate and create a non blocking channel between our application and the _AWS S3 service_. 
 This, the api is exposed under the `monix.connect.s3.S3` object and they all expects an _implicit_ instance of 
 this async client class to be in the scope of the call.
 
 Below code shows is just an example on how to set up this connection to the AWS S3, you must configure properly to your use case. 
 See in how to use it locally in [the last section](https://connect.monix.io/docs/s3#local-testing) of this page.
 Note that in this case the authentication is using AWS access and secret keys, but you might use another method such an _IAM_ role.
 
 ```scala
import java.time.Duration

import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient

/* exceptions related with concurrency or timeouts from any of the requests
 * might be solved by raising the maxConcurrency, maxPendingConnectionAcquire or
 * connectionAcquisitionTimeout from the underlying netty http client.
 * see in below code how do so, the values are just example ones.
 */
val httpClient = NettyNioAsyncHttpClient.builder()
  .maxConcurrency(500)
  .maxPendingConnectionAcquires(50000)
  .connectionAcquisitionTimeout(Duration.ofSeconds(60))
  .readTimeout(Duration.ofSeconds(60))
  .build()

// note that the client is defined as implicit, and this is on purpose since 
// each of the methods will expect it in that way.
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .httpClient(httpClient) //optional and needs to be properly configured
  .credentialsProvider(DefaultCredentialsProvider.create())
  .region(AWS_GLOBAL)
  .build
```


### Create bucket 

Once you have configured the client, you would probably need to _create a bucket_:

 ```scala
import software.amazon.awssdk.services.s3.model.CreateBucketResponse
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val t: Task[CreateBucketResponse] = S3.createBucket(bucket)
```
## Delete bucket 

 On the other hand if you want to remove the bucket:
 
 ```scala
import software.amazon.awssdk.services.s3.model.DeleteBucketResponse
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val t: Task[DeleteBucketResponse] = S3.deleteBucket(bucket)
```

## Delete object
  
You can also delete an _object_ within the specified _bucket_ with:
 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 

val t: Task[DeleteObjectResponse] = S3.deleteObject(bucket, key)
```

## Exists bucket
  
Check whether the specified bucket exists or not.

 ```scala
import monix.connect.s3.S3

val bucket: String = "my-bucket" 

val t: Task[Boolean] = S3.existsBucket(bucket)
```


## Exists object
  
Check whether the specified object exists or not.

 ```scala
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 


val t: Task[Boolean] = S3.existsObject(bucket, key)
```

## List objects
  
Lists the objects within a _bucket_ and _prefix_.

 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import software.amazon.awssdk.services.s3.model.S3Object
import monix.connect.s3.S3
import monix.reactive.Observable

val bucket = "my-bucket"
val prefix = s"prefix/to/list/keys/"

val s3Objects: Observable[S3Object] = S3.listObjects(bucket, prefix = Some(prefix))

// use `maxTotalKeys` to set a limit to the number of keys you want to fetch
val s3ObjectsWithLimit: Observable[S3Object] = 
  S3.listObjects(bucket, maxTotalKeys = Some(1011), prefix = Some(prefix))
```

Listing a very long list of objects might take time and and cause the underlying netty connection to error because 
of the request timeout, in order increase that see fix that you can increase those timeouts with a new configuration for `software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient`
that will be added to the `software.amazon.awssdk.services.s3.S3AsyncClient`. _See_ an example _Async Client_ [section](##Async Client)


### Download
  
Downloads the specified object in a single request as _byte array_. 

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/download/path/file.txt"
val content: Array[Byte] = "Dummy file content".getBytes()

val t: Task[Array[Byte]] = S3.download("my-bucket", key)
```

Note that this operation is only suitable to be used for small objects,
  for large ones `downloadMultipart` should be used.
  
  On contrast, it might be useful in those cases in which the user 
  only wants to download the first _n_ bytes from an object:
  
```scala
  // only downloads the first 10 bytes
  S3.download("my-bucket", key, firstNBytes = Some(10))
```
  
### Download multipart
  
A method that _safely_ downloads objects of any size by performing _partial download requests_.
The number of bytes to get per each request is specified by the `chunkSize`.

 ```scala
import monix.connect.s3.S3
import monix.reactive.Observable

val bucket = "my-bucket"
val key = "sample/path/file.json"

val ob: Observable[Array[Byte]] = S3.downloadMultipart("my-bucket", key, chunkSize = 5242880)
```

## Upload

You can also easily upload an object into an _S3_  with the _upload_ signature.
Note that if you need to update large amount of data you should not be using this method, see instead [multipartUpload](https://connect.monix.io/docs/s3#multipart-upload)`.

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/upload/path/file.txt"
val content: Array[Byte] = "Dummy file content".getBytes()

val t: Task[PutObjectResponse] = S3.upload("my-bucket", key, content)
```

## Upload multipart

When dealing with large files of data you should use the `multipartUpload` operation.
This one can be used to consume an observable of bytes while sending partial upload request, if chunk was smaller than the minimum size it will be concatenated with the next request. 

Thus, it reduces substantially the risk on having _OOM_ errors or failed _http requests_ due to the size of the body.

Note that the method can be tuned with specific _aws configurations_ such as `acl`, `requestPayer`, etc. 
And most importantly, it allows to set `chunksize`, in which the _default_ and _minimum_ value is _5MB_ or _5242880 bytes_.


```scala
import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse
import monix.connect.s3.S3
import monix.reactive.Observable
import monix.reactive.Consumer
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/upload/path/file.txt"

// preasumably we have a stream of bytes coming in
val ob = Observable.fromIterable(List("A", "B", "C").map(_.getBytes))

// and a multipart upload consumer
val uploadConsumer: Consumer[Array[Byte], Task[CompleteMultipartUploadResponse]] =
  S3.uploadMultipart(bucket, key)

// then
val r: Task[CompleteMultipartUploadResponse] = ob.consumeWith(uploadConsumer)
```


## Local testing

There is actually a very good support on regards to testing `AWS S3` locally, the following sections describes different popular alternatives:
 
### Localstack

 A fully functional local _AWS_ cloud stack available as a docker image.
 
 You would just need to define it as a service in your `docker-compose.yml`:
 
 ```yaml
 localstack:
    image: localstack/localstack:0.11.0
    hostname: localstack
    container_name: localstack
    ports:
      - '4566:4566'
    environment:
      - SERVICES=s3
# very important to specify `s3` on `SERVICES` env var, it would prevent to spin up the rest of the AWS services.
``` 

 Then, execute the following command to build and run the _localstack_ image:
 
 ```shell script
 docker-compose -f ./docker-compose.yml up -d localstack
```

A good point on favor to using _localstack_ is that it provides support for _AWS Anonymous Credentials_, meaning that you can easily connect to your 
local _S3 service_ with no required authentication. 
 
See below an example on how create the _async client_:
 
 ```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider
import java.net.URI

val localStackEndpoint = "http://localhost:4566"
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(new AnonymousCredentialsProvider)
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(localStackEndpoint))
  .build
 ```
 _Notice_ that the the _client_ is defined as `implicit` since it is how the api will expect it.
 
**Important** - Whenever you create a bucket on _localstack_, you would better set its _ACL (Access Control List)_ as `public-read` since it might prevent you to encounter [403 access denied](https://github.com/localstack/localstack/issues/406) when reading it back.
If you set the `container_name` field to _localstack_ in `docker-compose.yaml` you can create the bucket and specify the right _ACL_ like:
```shell script
docker exec localstack awslocal s3 mb s3://my-bucket
docker exec localstack awslocal s3api put-bucket-acl --bucket my-bucket --acl public-read
``` 
On the other hand, if prefer to do that from code:

```scala
import monix.connect.s3.S3
import org.scalatest.BeforeAndAfterAll

override def beforeAll() = {
  super.beforeAll()
  S3.createBucket("my-bucket", acl = Some("public-read")).runSyncUnsafe()
}
```

### Minio
 
[Minio](https://github.com/minio/minio) is another well known docker image that emulates _AWS S3_.

The advantages of using _minio_ over _localstack_ is that it provides a nice _UI_ that allows you to quickly visualize and manage the 
 objects and buckets stored in your local, instead of having to _exec_ in your local image.
 
 On the other hand, a disadvantage could be that it does not support _Anonymous Credentials_, so you have to specify _key_ and _secret_ to create the connection.
 
Add the following service description to your `docker-compose.yaml` file:

```yaml
minio:
  image: minio/minio
  ports:
    - "9000:9000"
  volumes:
    - ./minio/data:/data
  environment:
    - MINIO_ACCESS_KEY=TESTKEY
    - MINIO_SECRET_KEY=TESTSECRET
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 35s
    timeout: 20s
    retries: 3
  command: server --compat /data
```

Then, run the following command to build and start the _minio_ image:

```shell script
docker-compose -f ./docker-compose.yml up -d minio
``` 

Check out that the service has started correctly, notice that there is a _healthcheck_ on the definition of the _minio service_, 
that's because it is a heavy image and sometimes it takes bit long to start or it even fails, so by adding it will prevent that to happen.

Finally, now you can already create the connection to _AWS S3_, _notice_ that _minio_ does not support _Anonymous credentials_, instead you'll have to use _Basic Credentials_ and specify the _key_ and _secret_ corresponding respectively to the
 defined environment variables `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY`.

```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import java.net.URI

val minioEndPoint: String = "http://localhost:9000"

val s3AccessKey: String = "TESTKEY" //equal to the env var `MINIO_ACCESS_KEY`  
val s3SecretKey: String = "TESTSECRET" //equal to the `env var `MINIO_SECRET_KEY`

val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(StaticCredentialsProvider.create(basicAWSCredentials))
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(minioEndPoint))
  .build
```

### JVM S3 Mock library

In case you prefer to _start_ and _stop_ the _S3_ service from from the code of same test and therefore not depending on _docker_ but just on a _JVM library dependency_, you can refer to [findify/s3Mock](https://github.com/findify/s3mock) to see more. 
