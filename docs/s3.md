---
id: s3
title: AWS S3
---

## Introduction

_AWS Simple Storage Service_ ([S3](https://aws.amazon.com/s3/?nc=sn&loc=0)) is an object storage service that offers industry leading scalability, availability, security and performance.
It allows data storage of any amount of data, commonly used as a data lake for big data applications which can now be easily integrated with monix.
   
## Dependency
 
 Add the following dependency:
 
 ```scala
 libraryDependencies += "io.monix" %% "monix-s3" % "0.3.3"
 ```

## Getting started 


 First of all, we need to create the s3 client that will allow us to authenticate and create an channel between our 
 application and the AWS S3 service. 
 
 ### asyncClient
 
 This module has been implemented using the `S3AsyncClient` from the [aws sdk](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/package-summary.html), since it only exposes non blocking methods. 
 So, all of the methods exposed under `monix.connect.s3.S3` object would expect an _implicit_ instance of 
 this async client class to be in the scope of the call.
 
 Below code shows an example on how to set up this connection. 
 Note that in this case the authentication is using AWS access and secret keys, but you might use another method such an _IAM_ role.
 
 ```scala
import java.net.URI
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.regions.Region.AWS_GLOBAL

val basicAWSCredentials: AwsBasicCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
val credentialsProvider: StaticCredentialsProvider = StaticCredentialsProvider.create(basicAWSCredentials)

//note that the client is defined as implicit, this is on purpose since each of the methods defined in
//the monix s3 connector will expect that.
 implicit val s3Client: S3AsyncClient = S3AsyncClient
    .builder()
    .credentialsProvider(credentialsProvider)
    .region(AWS_GLOBAL)
    .endpointOverride(URI.create(endPoint)) //this one is used to point to the localhost s3 service, not used in prod 
    .build
```


  ### createBucket 

Once you have configured the client, you would probably need to create a bucket:

 ```scala
import software.amazon.awssdk.services.s3.model.CreateBucketResponse

val bucketName: String = "myBucket" 
val t: Task[CreateBucketResponse] = S3.createBucket(bucketName)
```
  ### deleteBucket 

 On the other hand if you want to remove the bucket:
 
 ```scala
import software.amazon.awssdk.services.s3.model.DeleteBucketResponse

val bucketName: String = "myBucket" 
val t: Task[DeleteBucketResponse] = S3.deleteBucket(bucketName)
```

  ### putObject

You can also easily create and write into an S3 object with put object operation.
Note that if you need to update large amount of data you should not be using this method, see instead [multipartUpload](###m)`.

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse

val content: Array[Byte] = "file content".getBytes()
val t: Task[PutObjectResponse] = S3.putObject(bucketName, objectKey, content)
}
```

  ### deleteObject
  
You can also operate at object level within a bucket with:
 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}

val t: Task[DeleteObjectResponse] = S3.deleteObject(bucketName)
```

  ### listObject
  
Lists all the objects within a bucket:

 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}

val _: Task[ListObjectsResponse] = S3.listObjects(bucketName)
```

  ### getObject
  
Download the given S3 object as a single byte array. 
Note that this operation is dangerous to perform on large objects, `multipartDownload` would be supported in future releases to 
support those use cases.

 ```scala
val objectKey: String = "/object/file.txt"
val _: Task[Array[Byte]] = S3.getObject(bucketName, objectKey)

```

### multipartUpload

Finally, for dealing with large files of data you might want to use the `multipartUpload` implementation.
This one can be used to consume an observable of bytes that would send a partial upload request for each received element if it was bigger than the minimum size, otherwise the chunk will be concatenated on the next request. 

Thus, it reduces substantially the risk on having _OOM_ errors or getting http requests failures, 
since the whole file does not need to be allocated in the memory and the http request body won't be that big because it would have been done by parts. 

Note that the method can be tuned with specific aws configurations such as `acl's`, `requestPayer`, etc. But a very important one to have present is by the minimum `chunksize` that will be sent, being 5MB the default and minimum size (lower values would result in failure).

```scala
import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse

// given an stream of array bytes 
val ob: Observable[Array[Byte]] = Observable.fromIterable(chunks)

// and a multipart upload consumer
val multipartUploadConsumer: Consumer[Array[Byte], Task[CompleteMultipartUploadResponse]] =
  S3.multipartUpload(bucketName, objectKey)

// then
ob.fromIterable(chunks).consumeWith(multipartUploadConsumer)
```

## Local testing

There is actually a very good support on regards testing `AWS S3`, you could either spin up a docker image to just use a JVM library that emulates such service.

The following sections describes different alternatives:
 
### Localstack

 A fully functional local _AWS_ cloud stack available as a docker image.
 
 You would just need to define it as a service in your `docker-compose.yml`:
 
 ```yaml
 localstack:
    image: localstack/localstack:latest
    hostname: localstack
    container_name: localstack
    ports:
      - '4566:4566'
    environment:
      - SERVICES=s3
# very important to specify `s3` on `SERVICES` env var, it would prevent to spin up the rest of the AWS services.
``` 

 Then, run the following command to build and start the _S3_ service:
 
 ```shell script
 docker-compose -f ./docker-compose.yml up -d localstack
```

A good point on favor to using _localstack_ is that it provides support for _AWS Anonymous Credentials_, meaning that you can easily connect to your 
local S3 service with no required authentication. 
 
See below an example on how create the async client:
 
 ```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider
import java.net.URI

val localStackEndpoint = "http://localhost:4566"
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(new AnonymousCredentialsProvider)
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(localStackEndpoint))
  .build
 ```
 _Notice_ that the the _client_ was defined as `implicit` since it is how the api will expect it.
 
**Important** - Whenever you create a bucket on localstack, you would better set its _ACL (Access Control List)_ as `public-read` since it might prevent you to encounter [403 access denied](localstack/localstack#406) when reading.
If you set the `container_name` field to _localstack_ in `docker-compose.yaml` you can create the bucket and specify the right _ACL_ like:
```shell script
docker exec localstack awslocal s3 mb s3://my-bucket
docker exec localstack awslocal s3api put-bucket-acl --bucket my-bucket --acl public-read
``` 
On the other hand, if prefer to do that from code:

```scala
import monix.connect.s3.S3
import org.scalatest.BeforeAndAfterAll

override def beforeAll() = {
  super.beforeAll()
  S3.createBucket("my-bucket", acl = Some("public-read")).runSyncUnsafe()
}
```

### Minio
 
[Minio](https://github.com/minio/minio) is another well known docker image that emulates _AWS S3_.

The advantages of using _minio_ over _localstack_ is that it provides a beautiful _UI_ that allows you to quickly visualize and manage the 
 objects and buckets stored in the local S3. 
 On the other hand, a disadvantage could be that it does not support _Anonymous Credentials_, so you have to specify _key_ and _secret_ to create the connection.
 
The following service description to your `docker-compose.yaml` file:

```yaml
minio:
  image: minio/minio
  ports:
    - "9000:9000"
  volumes:
    - ./minio/data:/data
  environment:
    - MINIO_ACCESS_KEY=TESTKEY
    - MINIO_SECRET_KEY=TESTSECRET
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 35s
    timeout: 20s
    retries: 3
  command: server --compat /data
```

Then, run the following command to build and start the S3 service:

```shell script
docker-compose -f ./docker-compose.yml up -d minio
``` 

Check out that the service has started correctly, notice that a _healthcheck_ has been defined on the description of the minio service, 
that's because it is a heavy image and sometimes it takes bit long to start or it even fails, so adding it will prevent that to happen.

Finally you can already create the connection to _AWS S3_, _notice_ that _minio_ does not support _Anonymous credentials_, instead you'll have to use _Basic Credentials_ and specify the _key_ and _secret_ corresponding respectively to the
 defined environment variables `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY`.

```scala
import software.amazon.awssdk.regions.Region.AWS_GLOBAL
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import java.net.URI

val minioEndPoint: String = "http://localhost:9000"

val s3AccessKey: String = "TESTKEY" //equal to the env var `MINIO_ACCESS_KEY`  
val s3SecretKey: String = "TESTSECRET" //equal to the `env var `MINIO_SECRET_KEY`

val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
implicit val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(StaticCredentialsProvider.create(basicAWSCredentials))
  .region(AWS_GLOBAL)
  .endpointOverride(URI.create(minioEndPoint))
  .build
```


### JVM S3 Mock library

In case you prefer to _start_ and _stop_ the _S3_ service from from the code of same test and therefore not depending on _docker_ but just on a _JVM library dependency_, you can refer to [findify/s3Mock](https://github.com/findify/s3mock) to see more. 
